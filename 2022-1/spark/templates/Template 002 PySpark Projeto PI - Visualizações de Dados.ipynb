{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cdabdf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Continuando do Template 001\n",
    "# Explorando a Visaulização de Dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fe588",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Devemos incializar o serviço do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2e5eb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3857bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/25 20:35:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7b3f3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Trazendo um exemplo de DataFrame (template001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee8f3b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='marise', d=date(2022, 5, 20), e=datetime(2022, 5, 20, 19, 0)),\n",
    "    Row(a=2, b=3., c='alex', d=date(2022, 5, 21), e=datetime(2022, 5, 21, 20, 0)),\n",
    "    Row(a=3, b=4., c='chola', d=date(2022, 5, 22), e=datetime(2022, 5, 22, 21, 0))\n",
    "])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee9b25",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Visualizando a primeira linha do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3098cd17",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+\n",
      "|  a|  b|     c|         d|                  e|\n",
      "+---+---+------+----------+-------------------+\n",
      "|  1|2.0|marise|2022-05-20|2022-05-20 19:00:00|\n",
      "+---+---+------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d24194",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+\n",
      "|  a|  b|     c|         d|                  e|\n",
      "+---+---+------+----------+-------------------+\n",
      "|  1|2.0|marise|2022-05-20|2022-05-20 19:00:00|\n",
      "|  2|3.0|  alex|2022-05-21|2022-05-21 20:00:00|\n",
      "|  3|4.0| chola|2022-05-22|2022-05-22 21:00:00|\n",
      "+---+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67380772",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+\n",
      "|  a|  b|     c|         d|                  e|\n",
      "+---+---+------+----------+-------------------+\n",
      "|  1|2.0|marise|2022-05-20|2022-05-20 19:00:00|\n",
      "|  2|3.0|  alex|2022-05-21|2022-05-21 20:00:00|\n",
      "|  3|4.0| chola|2022-05-22|2022-05-22 21:00:00|\n",
      "+---+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f544de7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | marise              \n",
      " d   | 2022-05-20          \n",
      " e   | 2022-05-20 19:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 3.0                 \n",
      " c   | alex                \n",
      " d   | 2022-05-21          \n",
      " e   | 2022-05-21 20:00:00 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d277ac68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | marise              \n",
      " d   | 2022-05-20          \n",
      " e   | 2022-05-20 19:00:00 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63d4ff42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20ab355",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b16c5f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exibir uma estatística resumida do DataFrame = equivalente ao summary\n",
    "Observe stddev - desvio padrão "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be797f6b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+\n",
      "|summary|  a|  b|     c|\n",
      "+-------+---+---+------+\n",
      "|  count|  3|  3|     3|\n",
      "|   mean|2.0|3.0|  null|\n",
      "| stddev|1.0|1.0|  null|\n",
      "|    min|  1|2.0|  alex|\n",
      "|    max|  3|4.0|marise|\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"a\", \"b\", \"c\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87f479",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Exibir uma coleção dados distribuídos dos quais o tamanho da  memória RAM pode não suportar. Aqui apenas como um exemplo de como fazer as seleções, já que a massa de df é ínfima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f294703",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='marise', d=datetime.date(2022, 5, 20), e=datetime.datetime(2022, 5, 20, 19, 0)),\n",
       " Row(a=2, b=3.0, c='alex', d=datetime.date(2022, 5, 21), e=datetime.datetime(2022, 5, 21, 20, 0)),\n",
       " Row(a=3, b=4.0, c='chola', d=datetime.date(2022, 5, 22), e=datetime.datetime(2022, 5, 22, 21, 0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3697c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para evitar a falta de memória podemos usar take ou tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9de516f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='marise', d=datetime.date(2022, 5, 20), e=datetime.datetime(2022, 5, 20, 19, 0))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3642f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='marise', d=datetime.date(2022, 5, 20), e=datetime.datetime(2022, 5, 20, 19, 0)),\n",
       " Row(a=2, b=3.0, c='alex', d=datetime.date(2022, 5, 21), e=datetime.datetime(2022, 5, 21, 20, 0))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce682807",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='marise', d=datetime.date(2022, 5, 20), e=datetime.datetime(2022, 5, 20, 19, 0)),\n",
       " Row(a=2, b=3.0, c='alex', d=datetime.date(2022, 5, 21), e=datetime.datetime(2022, 5, 21, 20, 0)),\n",
       " Row(a=3, b=4.0, c='chola', d=datetime.date(2022, 5, 22), e=datetime.datetime(2022, 5, 22, 21, 0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e63b0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='marise', d=datetime.date(2022, 5, 20), e=datetime.datetime(2022, 5, 20, 19, 0)),\n",
       " Row(a=2, b=3.0, c='alex', d=datetime.date(2022, 5, 21), e=datetime.datetime(2022, 5, 21, 20, 0)),\n",
       " Row(a=3, b=4.0, c='chola', d=datetime.date(2022, 5, 22), e=datetime.datetime(2022, 5, 22, 21, 0))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71545e52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "O PySpark DataFrame também fornece a conversão de volta para um DataFrame pandas para aproveitar a API do pandas. Observe que toPandastambém coleta todos os dados no lado do driver que podem facilmente causar um erro de falta de memória quando os dados são muito grandes para caber no lado do driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7a277e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>marise</td>\n",
       "      <td>2022-05-20</td>\n",
       "      <td>2022-05-20 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>alex</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2022-05-21 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>chola</td>\n",
       "      <td>2022-05-22</td>\n",
       "      <td>2022-05-22 21:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b       c           d                   e\n",
       "0  1  2.0  marise  2022-05-20 2022-05-20 19:00:00\n",
       "1  2  3.0    alex  2022-05-21 2022-05-21 20:00:00\n",
       "2  3  4.0   chola  2022-05-22 2022-05-22 21:00:00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e86b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Selecionando e Acessando dados\n",
    "O modo de operação do PySpark DataFrame é diferente de outras Tools, ao selecionar uma coluna da tabela, retorna uma instância dessa coluna, vejamos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c325e4bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a7f5c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Desta maneira, precisamos então ao usar colunas, outro DataFrame será gerado. Vejamos a seguir quando selecionado a coluna c.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7eb250c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8663871",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     c|\n",
      "+------+\n",
      "|marise|\n",
      "|  alex|\n",
      "| chola|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.c).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac39091",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Vamos atribuir uma nova coluna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3fb50f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+-------+\n",
      "|  a|  b|     c|         d|                  e|upper_c|\n",
      "+---+---+------+----------+-------------------+-------+\n",
      "|  1|2.0|marise|2022-05-20|2022-05-20 19:00:00| MARISE|\n",
      "|  2|3.0|  alex|2022-05-21|2022-05-21 20:00:00|   ALEX|\n",
      "|  3|4.0| chola|2022-05-22|2022-05-22 21:00:00|  CHOLA|\n",
      "+---+---+------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('upper_c', upper(df.c)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee217920",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+\n",
      "|  a|  b|     c|         d|                  e|\n",
      "+---+---+------+----------+-------------------+\n",
      "|  1|2.0|marise|2022-05-20|2022-05-20 19:00:00|\n",
      "|  2|3.0|  alex|2022-05-21|2022-05-21 20:00:00|\n",
      "|  3|4.0| chola|2022-05-22|2022-05-22 21:00:00|\n",
      "+---+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e60f8a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Vc viu a diferença de uma para o outro? A coluna Upper foi instãnciada ao DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42869ef6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+\n",
      "|  a|  b|     c|         d|                  e|\n",
      "+---+---+------+----------+-------------------+\n",
      "|  1|2.0|marise|2022-05-20|2022-05-20 19:00:00|\n",
      "+---+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29c40f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aplicando uma função\n",
    "\n",
    "Apache Arrow é um formato de dados colunar na memória que é usado no Spark para transferir dados com eficiência entre os processos JVM e Python.\n",
    "\n",
    "Scalar Pandas UDFs são usados ​​para vetorizar operações escalares. Para definir uma UDF de Pandas escalar, basta usar @pandas_udfpara anotar uma função Python que recebe como argumentos e retorna outra do mesmo tamanho. Abaixo ilustramos usando dois exemplos: Mais Um e Probabilidade Cumulativa.pandas.Seriespandas.Series. \n",
    "A computação v + 1 é um exemplo simples para demonstrar as diferenças entre UDFs de linha por vez e UDFs Pandas escalares.\n",
    "\n",
    "Para saber mais: https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "\n",
    "Veja o template004 de Exemplos de aplciações. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d4b47c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5546b658",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(a + 1)|\n",
      "+-------+\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "df.select(pandas_plus_one(df.a)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae46f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Agrupamento de dados\n",
    "O PySpark DataFrame também fornece uma maneira de lidar com dados agrupados usando a abordagem comum, a estratégia split-apply-combine. Ele agrupa os dados por uma determinada condição, aplica uma função a cada grupo e depois os combina de volta ao DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28911d67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['amarelo', 'banana', 1, 10], ['azul','banana', 2, 20], ['vermelho', 'cenoura', 3, 30],\n",
    "    ['azul','uva', 4, 40], ['vermelho', 'cenoura', 5, 50], ['marron', 'cenoura', 6, 60], \n",
    "    ['vermelho', 'banana', 7, 70], ['vermelho', 'uva', 8, 80]], schema=['cor', 'fruta', 'qte1', 'qte2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8018759c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+----+\n",
      "|     cor|  fruta|qte1|qte2|\n",
      "+--------+-------+----+----+\n",
      "| amarelo| banana|   1|  10|\n",
      "|    azul| banana|   2|  20|\n",
      "|vermelho|cenoura|   3|  30|\n",
      "|    azul|    uva|   4|  40|\n",
      "|vermelho|cenoura|   5|  50|\n",
      "|  marron|cenoura|   6|  60|\n",
      "|vermelho| banana|   7|  70|\n",
      "|vermelho|    uva|   8|  80|\n",
      "+--------+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477a1cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cálculo da média aritmética com uma função do spark.\n",
    "Aplicando um comando para calcular a média aritmética - Average - aplicando a seleção pro agrupamento de cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9332ac4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "|     cor|avg(qte1)|avg(qte2)|\n",
      "+--------+---------+---------+\n",
      "|vermelho|     5.75|     57.5|\n",
      "|    azul|      3.0|     30.0|\n",
      "| amarelo|      1.0|     10.0|\n",
      "|  marron|      6.0|     60.0|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('cor').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2a2b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Obtendo dados de entrada e saída CSV\n",
    "\n",
    "CSV é simples e fácil de usar. Parquet e ORC são formatos de arquivo eficientes e compactos para ler e escrever mais rápido.\n",
    "\n",
    "Existem muitas outras fontes de dados disponíveis no PySpark, como JDBC, text, binaryFile, Avro, etc. Consulte também o Spark SQL, DataFrames and Datasets Guide mais recente na documentação do Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b0365d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+----+\n",
      "|     cor|  fruta|qte1|qte2|\n",
      "+--------+-------+----+----+\n",
      "|vermelho|cenoura|   5|  50|\n",
      "|  marron|cenoura|   6|  60|\n",
      "|vermelho| banana|   7|  70|\n",
      "|vermelho|    uva|   8|  80|\n",
      "| amarelo| banana|   1|  10|\n",
      "|    azul| banana|   2|  20|\n",
      "|vermelho|cenoura|   3|  30|\n",
      "|    azul|    uva|   4|  40|\n",
      "+--------+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.csv('fruits.csv', header=True)\n",
    "spark.read.csv('fruits.csv', header=True).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}